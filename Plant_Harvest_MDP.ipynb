{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Example of MDP Approach to Unified Planting and Harvesting Optimization\n",
    "This is a minimal implementation of a Markov decision process formulation of the corn planting and harvesting model. This minimal example considers a limited set of fields which to produce corn undergo three different operations: planting, detasseling, and harvesting. Each operation is limited by its own daily capacity, operational time window, and growth degree unit (GDU). It is assumed that harvesting results in a reward of type yield which is a function of GDU at the time of harvest and the field's area. Below, a dynamic programming formulation of the problem is presented:\n",
    "\n",
    "## Dynamic Programming Framework\n",
    "**Assumptions**:\n",
    "\n",
    "    1. Different fields are not batched together\n",
    "    2. Only one hybrid type is considered\n",
    "    3. Sequence of operations: Planting --> Detasseling --> Harvesting\n",
    "    4. Unit of time for each decision epoch is a day\n",
    "    5. One operation per day on each field\n",
    "    6. It is assumed if the operation starts on a field, it will be finished by the end of the day\n",
    "    7. Operations are non-preemptive\n",
    "    8. GDU evolution is deterministic (by an average of 25 after planting)\n",
    "    9. Volume yield of a field is a function of GDU at the time of harvest\n",
    "    10. Fields are not prioritized\n",
    "    \n",
    "**Formulation**:\n",
    "\n",
    "    - Decision epochs: Discrete times for decision making\n",
    "$$t=0,\\ldots,T.$$\n",
    "\n",
    "    - State space: Summarizes the information neccessary for decision making\n",
    "$$X:=\\Big\\{x_t=\\{x^i_t\\} \\text{ where } x^i_t=(GDU^i_t, P^i_t, D^i_t, H^i_t) \\text{ for all } i=1,\\ldots,F, t=0,\\ldots,T\\Big\\},$$ where $GDU^i_t$ denotes the GDU of field $i$ at time $t$, $P^i_t\\in\\{0, 1\\}$ indicates whether field $i$ is planted by time $t$. $D^i_t$ and $H^i_t$ are also defined in a similar fashion.\n",
    "\n",
    "    - Action space: Available decisions at each state and decision epoch\n",
    "$$U_X(x_t):=\\Big\\{U_t=\\{U_{i,j}\\}\\in\\{0, 1\\} \\text{ for all } i=1,\\ldots,F, j\\in\\{p, d, h\\} \\text{ subject to } LU_t\\leq W \\text{ for all }t=0,\\ldots,T\\Big\\},$$ where $U_{i,j}=1$ if operation $j$ is assigned to field $i$. The action space is further restricted by linear constraints which can be viewed in matrix representation as $LU\\leq W$. These constraints are:\n",
    "\n",
    "        1. Daily resource capacities:\n",
    "$$\\sum_{i=1}^{F} U_{i,j}\\leq r_j, \\qquad\\qquad \\forall j\\in\\{p, d, h\\},$$ where $r^j$ denotes the daily capacity of resources for operation $j$.\n",
    "\n",
    "        2. Operational time windowns:\n",
    "$$U_{i,j} t \\leq \\overline{T}_j, \\qquad\\qquad \\forall j\\in\\{p, d, h\\},$$\n",
    "$$U_{i,j} \\underline{T}_j \\leq t, \\qquad\\qquad \\forall j\\in\\{p, d, h\\},$$ where $\\overline{T}_j$ denotes the latest date acceptable for operation $j$ while $\\underline{T}_j$ denotes the earliest date.\n",
    "\n",
    "        3. GDU range limits\n",
    "$$U_{i, j} GDU^i_t \\leq \\overline{GDU}_j, \\qquad\\qquad \\forall j\\in\\{d, h\\},$$\n",
    "$$U_{i, j}\\underline{GDU}_j \\leq GDU^i_t, \\qquad\\qquad \\forall j\\in\\{d, h\\},$$ where $\\overline{GDU}_j$ denotes the highest acceptable GDU for operation $j$ while $\\underline{GDU}_j$ denotes the lowest. Also, not that no GDU limits are required for planting since unless a field is planted, its GDU is zero!\n",
    "\n",
    "        4. Sequence of operations:\n",
    "$$\\begin{align*} &U_{i,p} P^i_t \\leq 0, \\qquad\\qquad &\\text { Do not plant field $i$ if already planted, i.e., } P^i_t=1,\\\\\n",
    "&U_{i,d} D^i_t \\leq 0, \\qquad\\qquad &\\text { Do not detassel field $i$ if already detasseled, i.e., } D^i_t=1,\\\\\n",
    "&U_{i,h} H^i_t \\leq 0, \\qquad\\qquad &\\text { Do not harvest field $i$ if already planted, i.e., } H^i_t=1,\\end{align*}$$\n",
    "and\n",
    "$$\\begin{align*} &U_{i,d}\\leq P^i_t, \\qquad\\qquad &\\text{ Do not detassel field $i$ if not planted,}\\\\\n",
    "&U_{i,h} \\leq D^i_t, \\qquad\\qquad &\\text{ Do not harvest field $i$ if not detasseled.}\\end{align*}$$\n",
    "\n",
    "        5. One operation per day:\n",
    "$$\\sum_{j\\in\\{p, d, h\\}} U_{i,j} \\leq 1, \\qquad\\qquad \\forall i=1,\\ldots,F.$$ Note that this constraint is redundant since the sequence of operation ensures that different field operations are done sequentially.\n",
    "\n",
    "\n",
    "    -Transition: Taking an action in any state results in a transition to another state\n",
    "$$\\begin{equation*} x_{t+1}=\\eta\\big(x_t, \\pi(x_t)\\big):=\\left\\{\\begin{array}{l}GDU^i_{t+1}=25(P^i_t-H^i_t)+GDU^i_t\\\\\n",
    "P^i_{t+1}=P^i_t+U_{i,p}\\\\\n",
    "D^i_{t+1}=D^i_t+U_{i,d}\\\\\n",
    "H^i_{t+1}=H^i_t+U_{i,h}\n",
    "\\end{array}\\right. \\qquad\\qquad \\forall i=1,\\ldots,F,\\end{equation*}$$ where $\\eta(\\cdot)$ is the transition function which identifies the future state $x_{t+1}$ as a function of the current state ${x_t}$ and the current decision $\\pi(x_t)$. Note that $\\pi(\\cdot)$ denotes the decision rule, i.e., policy, which determines the action $U_t$. Also, notice that GDU grows by 25 each day after a field is planted at time $t$ and stops growing once it is harvested. In addition, state variable $P^i_t$ (similarly $D^i_t$ and $H^i_t$) will be equal to 1 once planting (detasseling or harvesting) is started and will remain 1 to the end of the horizon $T$.\n",
    "\n",
    "    -Immediate reward: Once a field is harvested, an immediate reward of type yield is realized.\n",
    "$$\\begin{equation*} h\\big(x_t, \\pi(x_t)\\big)=\\left\\{\\begin{array}{ll}&0 &\\text{ if } U_{i,p}=1,\\\\\n",
    "&0 &\\text{ if } U_{i,d}=1,\\\\\n",
    "&Y(i, x_t) &\\text{ if } U_{i,h}=1,\n",
    "\\end{array}\\right. \\qquad\\qquad \\forall i=1,\\ldots,F,\\end{equation*}$$ where $h(\\cdot)$ is denotes the immediate reward of taking action $U_t$ under policy $\\pi$ at state $x_t$. It ia assumed for this implementation that planting and detasseling do not result in any form of costs, rewards, or utilities. However, once a field is harvested, its yield, i.e., $Y(i, x_t)$, is a function of the area of the field (hence argument $i$) and GDU at the time of harvest (hence argument $x_t$ which contains information about GDU of field $i$ in the form of $GDU^i_t$).\n",
    "\n",
    "**Optimality Equations**:\n",
    "\n",
    "The optimal policy is given by solving the following cumulative reward to optimality:\n",
    "$$V(x_0)=\\max_{\\pi\\in\\Pi} \\sum_{t=0}^{T} \\gamma^{T-t} h\\big(x_t, \\pi(x_t)\\big)$$ where $x_0$ denotes the initial state at time $0$. In this implementation, $x_0=[0]$ which is equivalent to assuming that none of the fields are planted at time $0$. Note that $\\Pi$ is the set of admissible policies, and $\\gamma$ is the discount factor which is smaller as we reach the end of horizon. In this implementation, $\\gamma$ is assumed to be 1.\n",
    "\n",
    "The recursion to solve this problem is based on Bellman's principle of optimality:\n",
    "\n",
    "$$\\begin{equation*}\\begin{array}{ll}&V(x_t)=\\max_{U_t\\in U_X(x_t)} h(x_t, U_t) + V(x_{t+1}), \\qquad\\qquad &\\forall t=0,\\ldots,T,\\\\\n",
    "&V(x_{T+1})=0, & \\end{array}\\end{equation*} $$ where the final reward at time $T+1$ is equal to zero since no operation will be allowed after the end of the horizon.\n",
    "\n",
    "## Backward Induction Code\n",
    "\n",
    "Since the reward at the end of the horizon is known, the code belwo recursively calculates $V(x_t)$ by evaluating $V(x_{t+1})$. The number of fields are limited to 4 for this example. After a solution is found, a Gantt chart is ploted to visualize the operations schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install memoization if necessary\n",
    "#!pip install memoization\n",
    "from memoization import cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, fields are defined. Area of each field is given. It is assumed that the whole operation from planting to harvesting is done within a 60 day period, and earliest and latest acceptable dates as well as lowest and highest acceptable GDUs for each operation are given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameter space\n",
    "fields=[1, 2, 3, 4]\n",
    "#fields volumes\n",
    "area_f=[10, 20, 15, 20]\n",
    "\n",
    "#horizon\n",
    "T=60\n",
    "\n",
    "#earliest & latest possible planting time\n",
    "EPT=0\n",
    "LPT=5\n",
    "\n",
    "#earliest & latest detasseling time\n",
    "EDT=30\n",
    "LDT=33\n",
    "\n",
    "#earliest & latest harvesting time\n",
    "EHT=58\n",
    "LHT=60\n",
    "\n",
    "#lowest & highest detasseling GDU\n",
    "LgduD=750\n",
    "HgduD=1000\n",
    "\n",
    "#lowest & highest harvesting GDU\n",
    "LgduH=1400\n",
    "HgduH=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state vector is assumed to be constructed of four list elements, i.e., x=[gdu, p, d, h] where, for example, ```len(gdu)==len(fields)```. Below, all permuations of 0 and 1 for $U_t=\\{U_{i,j}\\}, \\forall i=1,\\ldots,F, j\\in\\{p,d,h\\}$ are produced at first. Then, infeasible actions are filtered based on state $x_t$ at time $t$ according to the constraints defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the action space\n",
    "def action(x, t):\n",
    "    #decompose x\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    #initialize a set of potential actions. This is the permuation of all 0s and 1s.\n",
    "    u_p_lst=list(itertools.product([0,1],repeat=4))\n",
    "    u_d_lst=list(itertools.product([0,1],repeat=4))\n",
    "    u_h_lst=list(itertools.product([0,1],repeat=4))\n",
    "    \n",
    "    #1. Daily resource capacities:\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if sum(u_p)<=3] #planting reources <=3\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if sum(u_d)<=2] #detasseling resources <=2\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if sum(u_h)<=2] #harvesting resources <=2\n",
    "    \n",
    "    #2. Operational time windowns:\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if np.all(np.multiply(u_p, t)<= LPT)] #do not plant later than LPT\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if np.all(np.multiply(u_p, EPT)<= t)] #do not plant earlier than EPT\n",
    "    \n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, t)<= LDT)] #do not detassel later than LPT\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, EDT)<= t)] #do not detassel earlier than EPT\n",
    "    \n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, t)<= LHT)] #do not detassel later than LPT\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, EHT)<= t)] #do not detassel earlier than EPT\n",
    "    \n",
    "    #3. GDU range limits:\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, gdu)<= HgduD)] #do not detassel later than LPT\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, LgduD)<= gdu)] #do not detassel earlier than EPT\n",
    "    \n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, gdu)<= HgduH)] #do not detassel later than LPT\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, LgduH)<= gdu)] #do not detassel earlier than EPT\n",
    "    \n",
    "    #4. Sequence of operations:\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if np.all(np.multiply(u_p, p)<= 0)] #do not plant if already planted\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, d)<= 0)] #do not detassel if already detasseled\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, h)<= 0)] #do not harvest if already harvested\n",
    "    \n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.array(u_d)<=np.array(p))] #do not detassel if not planted\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.array(u_h)<=np.array(d))] #do not harvest if not detasseled\n",
    "    \n",
    "    #5. One operation per day:\n",
    "    ##maybe not neccessary because operational sequence constraints do not allow multiple operation in one day\n",
    "    \n",
    "    return[u_p_lst, u_d_lst, u_h_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the transition function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transition function\n",
    "def transition(x, u):\n",
    "    #decompose x, u\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    u=[list(i) for i in u] #this is necessary because u is of the form tuple\n",
    "    u_p=u[0]\n",
    "    u_d=u[1]\n",
    "    u_h=u[2]\n",
    "    \n",
    "    #transition\n",
    "    gdu=list(np.array(gdu)+25*(np.array(p)-np.array(h)))\n",
    "    p=list(np.array(p)+np.array(u_p))\n",
    "    d=list(np.array(d)+np.array(u_d))\n",
    "    h=list(np.array(h)+np.array(u_h))\n",
    "    \n",
    "    return [gdu, p, d, h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the immediate reward function is defined. It is assumed that maximum yield is only achieved at harvest GDU=1450. In addition, for every 1 unit of GDU difference at the time of harvest, 0.001 of yield is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define reward\n",
    "def reward(x, u):\n",
    "    #decompose x, u\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    u=[list(i) for i in u] #this is necessary because u is of the form tuple\n",
    "    u_p=u[0]\n",
    "    u_d=u[1]\n",
    "    u_h=u[2]\n",
    "    \n",
    "    reward=sum(np.multiply(np.multiply(area_f, 1-abs(np.array(gdu)-1450)/1000), u_h))\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the Bellman's principle of optimiality is implemented. Here, the $V(x_t)$ is kept in memory, ```@cached(max_size=None)``` for every $x_t$ to reduce the number of calculations in the recursion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bellman's principle\n",
    "@cached(max_size=None) #memoization\n",
    "def value_function(x, t):\n",
    "    #decompose s\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    if (t==T+1):\n",
    "        return 0\n",
    "    else:\n",
    "        #get the set of all available actions [(u_p1), (u_d), (u_h)]\n",
    "        u_set=action(x, t)\n",
    "        u_set=list(itertools.product(u_set[0], u_set[1], u_set[2]))\n",
    "        u_set=[list(i) for i in u_set]\n",
    "        \n",
    "        return max([reward(x, u)+value_function(transition(x, u), t+1) for u in u_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $x_0=[0]$ is defined, $t=0$ is set and $V(x_0)$ is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main recursion\n",
    "\n",
    "#define state variable\n",
    "gdu=[0,0,0,0]\n",
    "p=[0,0,0,0]\n",
    "d=[0,0,0,0]\n",
    "h=[0,0,0,0]\n",
    "x=[gdu, p, d, h]\n",
    "t=0\n",
    "\n",
    "v_star=value_function(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimal value function\n",
    "v_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the optimal action at every time is extracted based on the optimal value of $V(x_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=[]\n",
    "state_progression=[]\n",
    "gdu=[0,0,0,0]\n",
    "p=[0,0,0,0]\n",
    "d=[0,0,0,0]\n",
    "h=[0,0,0,0]\n",
    "x=[gdu, p, d, h]\n",
    "t=0\n",
    "for t in range(0, T+1):\n",
    "    u_set=action(x, t)\n",
    "    u_set=list(itertools.product(u_set[0], u_set[1], u_set[2]))\n",
    "    u_set=[list(i) for i in u_set]\n",
    "    new_value=[]\n",
    "    new_x=[]\n",
    "    for u in u_set:\n",
    "        x_temp=transition(x, u)\n",
    "        new_x.append(x_temp)\n",
    "        new_value.append(reward(x, u)+value_function(x_temp, t+1))\n",
    "    policy.append(u_set[np.argmax(new_value)])\n",
    "    x=new_x[np.argmax(new_value)]\n",
    "    state_progression.append(x)\n",
    "#policy.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0, 1, 1), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(1, 1, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 1, 1), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (1, 1, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 1, 1)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (1, 1, 0, 0)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy\n",
    "#[(u_p), (u_d), (u_h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[0, 0, 25, 25], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[25, 25, 50, 50], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[50, 50, 75, 75], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[75, 75, 100, 100], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[100, 100, 125, 125], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[125, 125, 150, 150], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[150, 150, 175, 175], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[175, 175, 200, 200], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[200, 200, 225, 225], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[225, 225, 250, 250], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[250, 250, 275, 275], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[275, 275, 300, 300], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[300, 300, 325, 325], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[325, 325, 350, 350], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[350, 350, 375, 375], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[375, 375, 400, 400], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[400, 400, 425, 425], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[425, 425, 450, 450], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[450, 450, 475, 475], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[475, 475, 500, 500], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[500, 500, 525, 525], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[525, 525, 550, 550], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[550, 550, 575, 575], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[575, 575, 600, 600], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[600, 600, 625, 625], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[625, 625, 650, 650], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[650, 650, 675, 675], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[675, 675, 700, 700], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[700, 700, 725, 725], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[725, 725, 750, 750], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[750, 750, 775, 775], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[775, 775, 800, 800], [1, 1, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0]],\n",
       " [[800, 800, 825, 825], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[825, 825, 850, 850], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[850, 850, 875, 875], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[875, 875, 900, 900], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[900, 900, 925, 925], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[925, 925, 950, 950], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[950, 950, 975, 975], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[975, 975, 1000, 1000], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1000, 1000, 1025, 1025], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1025, 1025, 1050, 1050], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1050, 1050, 1075, 1075], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1075, 1075, 1100, 1100], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1100, 1100, 1125, 1125], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1125, 1125, 1150, 1150], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1150, 1150, 1175, 1175], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1175, 1175, 1200, 1200], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1200, 1200, 1225, 1225], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1225, 1225, 1250, 1250], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1250, 1250, 1275, 1275], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1275, 1275, 1300, 1300], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1300, 1300, 1325, 1325], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1325, 1325, 1350, 1350], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1350, 1350, 1375, 1375], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1375, 1375, 1400, 1400], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1400, 1400, 1425, 1425], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1425, 1425, 1450, 1450], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1450, 1450, 1475, 1475], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 1, 1]],\n",
       " [[1475, 1475, 1475, 1475], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_progression\n",
    "#[[gdu], [p], [d], [h]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gantt_chart(policy):\n",
    "    #figure \"gnt\" \n",
    "    fig, gnt = plt.subplots()\n",
    "\n",
    "    #setting axis limits \n",
    "    gnt.set_ylim(0, len(fields)+1) \n",
    "    gnt.set_xlim(0, T+1+1)\n",
    "\n",
    "    #setting labels for x-axis and y-axis \n",
    "    gnt.set_xlabel('Days') \n",
    "    gnt.set_ylabel('Fields')\n",
    "\n",
    "    #ticks and labels on y-axis \n",
    "    gnt.set_yticks(fields)\n",
    "    gnt.set_yticklabels(['1', '2', '3', '4'])\n",
    "\n",
    "    #setting graph attribute \n",
    "    gnt.grid(True)\n",
    "\n",
    "    for i in range(0, len(policy)):\n",
    "        u_p=policy[i][0]\n",
    "        u_d=policy[i][1]\n",
    "        u_h=policy[i][2]\n",
    "        if (np.any(np.array(u_p)!=0)):\n",
    "            for j in range(0, len(fields)):\n",
    "                if (u_p[j]==1):\n",
    "                    gnt.broken_barh([(i, 1)], (j, 1), facecolors=('tab:green'))\n",
    "        if (np.any(np.array(u_d)!=0)):\n",
    "            for j in range(0, len(F)):\n",
    "                if (u_d[j]==1):\n",
    "                    gnt.broken_barh([(i, 1)], (j, 1), facecolors=('tab:orange'))\n",
    "        if (np.any(np.array(u_h)!=0)):\n",
    "            for j in range(0, len(F)):\n",
    "                if (u_h[j]==1):\n",
    "                    gnt.broken_barh([(i, 1)], (j, 1), facecolors=('tab:blue'))\n",
    "    return fig, gnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 432x288 with 1 Axes>,\n",
       " <AxesSubplot:xlabel='Days', ylabel='Fields'>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gantt_chart(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
