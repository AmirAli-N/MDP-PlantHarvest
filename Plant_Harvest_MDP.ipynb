{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Example of MDP Approach to Unified Planting and Harvesting Optimization\n",
    "This is a minimal implementation of a Markov decision process formulation of the corn planting and harvesting model. This minimal example considers a limited set of fields which to produce corn undergo three different operations: planting, detasseling, and harvesting. Each operation is limited by its own daily capacity, operational time window, and growth degree unit (GDU). It is assumed that harvesting results in a reward of type yield which is a function of GDU at the time of harvest and the field's area. Below, a dynamic programming formulation of the problem is presented:\n",
    "\n",
    "## Dynamic Programming Framework\n",
    "**Assumptions**:\n",
    "\n",
    "    1. Different fields are not batched together\n",
    "    2. Only one hybrid type is considered\n",
    "    3. Sequence of operations: Planting --> Detasseling --> Harvesting\n",
    "    4. Unit of time for each decision epoch is a day\n",
    "    5. One operation per day on each field\n",
    "    6. It is assumed if the operation starts on a field, it will be finished by the end of the day\n",
    "    7. Operations are non-preemptive\n",
    "    8. GDU evolution is deterministic (by an average of 25 after planting)\n",
    "    9. Volume yield of a field is a function of GDU at the time of harvest\n",
    "    10. Fields are not prioritized\n",
    "    \n",
    "**Formulation**:\n",
    "\n",
    "    - Decision epochs: Discrete times for decision making\n",
    "$$t=0,\\ldots,T.$$\n",
    "\n",
    "    - State space: Summarizes the information neccessary for decision making\n",
    "$$X:=\\Big\\{x_t=\\{x^i_t\\} \\text{ where } x^i_t=(GDU^i_t, P^i_t, D^i_t, H^i_t) \\text{ for all } i=1,\\ldots,F, t=0,\\ldots,T\\Big\\},$$ where $GDU^i_t$ denotes the GDU of field $i$ at time $t$, $P^i_t\\in\\{0, 1\\}$ indicates whether field $i$ is planted by time $t$. $D^i_t$ and $H^i_t$ are also defined in a similar fashion.\n",
    "\n",
    "    - Action space: Available decisions at each state and decision epoch\n",
    "$$U_X(x_t):=\\Big\\{U_t=\\{U_{i,j}\\}\\in\\{0, 1\\} \\text{ for all } i=1,\\ldots,F, j\\in\\{p, d, h\\} \\text{ subject to } LU_t\\leq W \\text{ for all }t=0,\\ldots,T\\Big\\},$$ where $U_{i,j}=1$ if operation $j$ is assigned to field $i$. The action space is further restricted by linear constraints which can be viewed in matrix representation as $LU\\leq W$. These constraints are:\n",
    "\n",
    "        1. Daily resource capacities:\n",
    "$$\\sum_{i=1}^{F} U_{i,j}\\leq r_j, \\qquad\\qquad \\forall j\\in\\{p, d, h\\},$$ where $r^j$ denotes the daily capacity of resources for operation $j$.\n",
    "\n",
    "        2. Operational time windowns:\n",
    "$$U_{i,j} t \\leq \\overline{T}_j, \\qquad\\qquad \\forall j\\in\\{p, d, h\\},$$\n",
    "$$U_{i,j} \\underline{T}_j \\leq t, \\qquad\\qquad \\forall j\\in\\{p, d, h\\},$$ where $\\overline{T}_j$ denotes the latest date acceptable for operation $j$ while $\\underline{T}_j$ denotes the earliest date.\n",
    "\n",
    "        3. GDU range limits\n",
    "$$U_{i, j} GDU^i_t \\leq \\overline{GDU}_j, \\qquad\\qquad \\forall j\\in\\{d, h\\},$$\n",
    "$$U_{i, j}\\underline{GDU}_j \\leq GDU^i_t, \\qquad\\qquad \\forall j\\in\\{d, h\\},$$ where $\\overline{GDU}_j$ denotes the highest acceptable GDU for operation $j$ while $\\underline{GDU}_j$ denotes the lowest. Also, not that no GDU limits are required for planting since unless a field is planted, its GDU is zero!\n",
    "\n",
    "        4. Sequence of operations:\n",
    "$$\\begin{align*} &U_{i,p} P^i_t \\leq 0, \\qquad\\qquad &\\text { Do not plant field $i$ if already planted, i.e., } P^i_t=1,\\\\\n",
    "&U_{i,d} D^i_t \\leq 0, \\qquad\\qquad &\\text { Do not detassel field $i$ if already detasseled, i.e., } D^i_t=1,\\\\\n",
    "&U_{i,h} H^i_t \\leq 0, \\qquad\\qquad &\\text { Do not harvest field $i$ if already planted, i.e., } H^i_t=1,\\end{align*}$$\n",
    "and\n",
    "$$\\begin{align*} &U_{i,d}\\leq P^i_t, \\qquad\\qquad &\\text{ Do not detassel field $i$ if not planted,}\\\\\n",
    "&U_{i,h} \\leq D^i_t, \\qquad\\qquad &\\text{ Do not harvest field $i$ if not detasseled.}\\end{align*}$$\n",
    "\n",
    "        5. One operation per day:\n",
    "$$\\sum_{j\\in\\{p, d, h\\}} U_{i,j} \\leq 1, \\qquad\\qquad \\forall i=1,\\ldots,F.$$ Note that this constraint is redundant since the sequence of operation ensures that different field operations are done sequentially.\n",
    "\n",
    "\n",
    "    -Transition: Taking an action in any state results in a transition to another state\n",
    "$$\\begin{equation*} x_{t+1}=\\eta\\big(x_t, \\pi(x_t)\\big):=\\left\\{\\begin{array}{l}GDU^i_{t+1}=25(P^i_t-H^i_t)+GDU^i_t\\\\\n",
    "P^i_{t+1}=P^i_t+U_{i,p}\\\\\n",
    "D^i_{t+1}=D^i_t+U_{i,d}\\\\\n",
    "H^i_{t+1}=H^i_t+U_{i,h}\n",
    "\\end{array}\\right. \\qquad\\qquad \\forall i=1,\\ldots,F,\\end{equation*}$$ where $\\eta(\\cdot)$ is the transition function which identifies the future state $x_{t+1}$ as a function of the current state ${x_t}$ and the current decision $\\pi(x_t)$. Note that $\\pi(\\cdot)$ denotes the decision rule, i.e., policy, which determines the action $U_t$. Also, notice that GDU grows by 25 each day after a field is planted at time $t$ and stops growing once it is harvested. In addition, state variable $P^i_t$ (similarly $D^i_t$ and $H^i_t$) will be equal to 1 once planting (detasseling or harvesting) is started and will remain 1 to the end of the horizon $T$.\n",
    "\n",
    "    -Immediate reward: Once a field is harvested, an immediate reward of type yield is realized.\n",
    "$$\\begin{equation*} h\\big(x_t, \\pi(x_t)\\big)=\\left\\{\\begin{array}{ll}&0 &\\text{ if } U_{i,p}=1,\\\\\n",
    "&0 &\\text{ if } U_{i,d}=1,\\\\\n",
    "&Y(i, x_t) &\\text{ if } U_{i,h}=1,\n",
    "\\end{array}\\right. \\qquad\\qquad \\forall i=1,\\ldots,F,\\end{equation*}$$ where $h(\\cdot)$ is denotes the immediate reward of taking action $U_t$ under policy $\\pi$ at state $x_t$. It ia assumed for this implementation that planting and detasseling do not result in any form of costs, rewards, or utilities. However, once a field is harvested, its yield, i.e., $Y(i, x_t)$, is a function of the area of the field (hence argument $i$) and GDU at the time of harvest (hence argument $x_t$ which contains information about GDU of field $i$ in the form of $GDU^i_t$).\n",
    "\n",
    "**Optimality Equations**:\n",
    "\n",
    "The optimal policy is given by solving the following cumulative reward to optimality:\n",
    "$$V(x_0)=\\max_{\\pi\\in\\Pi} \\sum_{t=0}^{T} \\gamma^{T-t} h\\big(x_t, \\pi(x_t)\\big)$$ where $x_0$ denotes the initial state at time $0$. In this implementation, $x_0=[0]$ which is equivalent to assuming that none of the fields are planted at time $0$. Note that $\\Pi$ is the set of admissible policies, and $\\gamma$ is the discount factor which is smaller as we reach the end of horizon. In this implementation, $\\gamma$ is assumed to be 1.\n",
    "\n",
    "The recursion to solve this problem is based on Bellman's principle of optimality:\n",
    "\n",
    "$$\\begin{equation*}\\begin{array}{ll}&V(x_t)=\\max_{U_t\\in U_X(x_t)} h(x_t, U_t) + V(x_{t+1}), \\qquad\\qquad &\\forall t=0,\\ldots,T,\\\\\n",
    "&V(x_{T+1})=0, & \\end{array}\\end{equation*} $$ where the final reward at time $T+1$ is equal to zero since no operation will be allowed after the end of the horizon.\n",
    "\n",
    "## Backward Induction Code\n",
    "\n",
    "Since the reward at the end of the horizon is known, the code belwo recursively calculates $V(x_t)$ by evaluating $V(x_{t+1})$. The number of fields are limited to 4 for this example. After a solution is found, a Gantt chart is ploted to visualize the operations schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install memoization if necessary\n",
    "#!pip install memoization\n",
    "from memoization import cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, fields are defined. Area of each field is given. It is assumed that the whole operation from planting to harvesting is done within a 60 day period, and earliest and latest acceptable dates as well as lowest and highest acceptable GDUs for each operation are given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameter space\n",
    "fields=[1, 2, 3, 4]\n",
    "#fields volumes\n",
    "area_f=[10, 20, 15, 20]\n",
    "\n",
    "#horizon\n",
    "T=60\n",
    "\n",
    "#earliest & latest possible planting time\n",
    "EPT=0\n",
    "LPT=5\n",
    "\n",
    "#earliest & latest detasseling time\n",
    "EDT=30\n",
    "LDT=33\n",
    "\n",
    "#earliest & latest harvesting time\n",
    "EHT=58\n",
    "LHT=60\n",
    "\n",
    "#lowest & highest detasseling GDU\n",
    "LgduD=750\n",
    "HgduD=1000\n",
    "\n",
    "#lowest & highest harvesting GDU\n",
    "LgduH=1400\n",
    "HgduH=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state vector is assumed to be constructed of four list elements, i.e., x=[gdu, p, d, h] where, for example, ```len(gdu)==len(fields)```. Below, all permuations of 0 and 1 for $U_t=\\{U_{i,j}\\}, \\forall i=1,\\ldots,F, j\\in\\{p,d,h\\}$ are produced at first. Then, infeasible actions are filtered based on state $x_t$ at time $t$ according to the constraints defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the action space\n",
    "def action(x, t):\n",
    "    #decompose x\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    #initialize a set of potential actions. This is the permuation of all 0s and 1s.\n",
    "    u_p_lst=list(itertools.product([0,1],repeat=4))\n",
    "    u_d_lst=list(itertools.product([0,1],repeat=4))\n",
    "    u_h_lst=list(itertools.product([0,1],repeat=4))\n",
    "    \n",
    "    #1. Daily resource capacities:\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if sum(u_p)<=3] #planting reources <=3\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if sum(u_d)<=2] #detasseling resources <=2\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if sum(u_h)<=2] #harvesting resources <=2\n",
    "    \n",
    "    #2. Operational time windowns:\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if np.all(np.multiply(u_p, t)<= LPT)] #do not plant later than LPT\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if np.all(np.multiply(u_p, EPT)<= t)] #do not plant earlier than EPT\n",
    "    \n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, t)<= LDT)] #do not detassel later than LPT\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, EDT)<= t)] #do not detassel earlier than EPT\n",
    "    \n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, t)<= LHT)] #do not detassel later than LPT\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, EHT)<= t)] #do not detassel earlier than EPT\n",
    "    \n",
    "    #3. GDU range limits:\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, gdu)<= HgduD)] #do not detassel later than LPT\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, LgduD)<= gdu)] #do not detassel earlier than EPT\n",
    "    \n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, gdu)<= HgduH)] #do not detassel later than LPT\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, LgduH)<= gdu)] #do not detassel earlier than EPT\n",
    "    \n",
    "    #4. Sequence of operations:\n",
    "    u_p_lst=[u_p for u_p in u_p_lst if np.all(np.multiply(u_p, p)<= 0)] #do not plant if already planted\n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.multiply(u_d, d)<= 0)] #do not detassel if already detasseled\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.multiply(u_h, h)<= 0)] #do not harvest if already harvested\n",
    "    \n",
    "    u_d_lst=[u_d for u_d in u_d_lst if np.all(np.array(u_d)<=np.array(p))] #do not detassel if not planted\n",
    "    u_h_lst=[u_h for u_h in u_h_lst if np.all(np.array(u_h)<=np.array(d))] #do not harvest if not detasseled\n",
    "    \n",
    "    #5. One operation per day:\n",
    "    ##maybe not neccessary because operational sequence constraints do not allow multiple operation in one day\n",
    "    \n",
    "    return[u_p_lst, u_d_lst, u_h_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the transition function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transition function\n",
    "def transition(x, u):\n",
    "    #decompose x, u\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    u=[list(i) for i in u] #this is necessary because u is of the form tuple\n",
    "    u_p=u[0]\n",
    "    u_d=u[1]\n",
    "    u_h=u[2]\n",
    "    \n",
    "    #transition\n",
    "    gdu=list(np.array(gdu)+25*(np.array(p)-np.array(h)))\n",
    "    p=list(np.array(p)+np.array(u_p))\n",
    "    d=list(np.array(d)+np.array(u_d))\n",
    "    h=list(np.array(h)+np.array(u_h))\n",
    "    \n",
    "    return [gdu, p, d, h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the immediate reward function is defined. It is assumed that maximum yield is only achieved at harvest GDU=1450. In addition, for every 1 unit of GDU difference at the time of harvest, 0.001 of yield is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define reward\n",
    "def reward(x, u):\n",
    "    #decompose x, u\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    u=[list(i) for i in u] #this is necessary because u is of the form tuple\n",
    "    u_p=u[0]\n",
    "    u_d=u[1]\n",
    "    u_h=u[2]\n",
    "    \n",
    "    reward=sum(np.multiply(np.multiply(area_f, 1-abs(np.array(gdu)-1450)/1000), u_h))\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the Bellman's principle of optimiality is implemented. Here, the $V(x_t)$ is kept in memory, ```@cached(max_size=None)``` for every $x_t$ to reduce the number of calculations in the recursion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bellman's principle\n",
    "@cached(max_size=None) #memoization\n",
    "def value_function(x, t):\n",
    "    #decompose s\n",
    "    gdu=x[0]\n",
    "    p=x[1]\n",
    "    d=x[2]\n",
    "    h=x[3]\n",
    "    \n",
    "    if (t==T+1):\n",
    "        return 0\n",
    "    else:\n",
    "        #get the set of all available actions [(u_p1), (u_d), (u_h)]\n",
    "        u_set=action(x, t)\n",
    "        u_set=list(itertools.product(u_set[0], u_set[1], u_set[2]))\n",
    "        u_set=[list(i) for i in u_set]\n",
    "        \n",
    "        return max([reward(x, u)+value_function(transition(x, u), t+1) for u in u_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $x_0=[0]$ is defined, $t=0$ is set and $V(x_0)$ is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main recursion\n",
    "\n",
    "#define state variable\n",
    "gdu=[0,0,0,0]\n",
    "p=[0,0,0,0]\n",
    "d=[0,0,0,0]\n",
    "h=[0,0,0,0]\n",
    "x=[gdu, p, d, h]\n",
    "t=0\n",
    "\n",
    "v_star=value_function(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimal value function\n",
    "v_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the optimal action at every time is extracted based on the optimal value of $V(x_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=[]\n",
    "state_progression=[]\n",
    "gdu=[0,0,0,0]\n",
    "p=[0,0,0,0]\n",
    "d=[0,0,0,0]\n",
    "h=[0,0,0,0]\n",
    "x=[gdu, p, d, h]\n",
    "t=0\n",
    "for t in range(0, T+1):\n",
    "    u_set=action(x, t)\n",
    "    u_set=list(itertools.product(u_set[0], u_set[1], u_set[2]))\n",
    "    u_set=[list(i) for i in u_set]\n",
    "    new_value=[]\n",
    "    new_x=[]\n",
    "    for u in u_set:\n",
    "        x_temp=transition(x, u)\n",
    "        new_x.append(x_temp)\n",
    "        new_value.append(reward(x, u)+value_function(x_temp, t+1))\n",
    "    policy.append(u_set[np.argmax(new_value)])\n",
    "    x=new_x[np.argmax(new_value)]\n",
    "    state_progression.append(x)\n",
    "#policy.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0, 1, 1), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(1, 1, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 1, 1), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (1, 1, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 1, 1)],\n",
       " [(0, 0, 0, 0), (0, 0, 0, 0), (1, 1, 0, 0)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy\n",
    "#[(u_p), (u_d), (u_h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[0, 0, 25, 25], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[25, 25, 50, 50], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[50, 50, 75, 75], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[75, 75, 100, 100], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[100, 100, 125, 125], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[125, 125, 150, 150], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[150, 150, 175, 175], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[175, 175, 200, 200], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[200, 200, 225, 225], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[225, 225, 250, 250], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[250, 250, 275, 275], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[275, 275, 300, 300], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[300, 300, 325, 325], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[325, 325, 350, 350], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[350, 350, 375, 375], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[375, 375, 400, 400], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[400, 400, 425, 425], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[425, 425, 450, 450], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[450, 450, 475, 475], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[475, 475, 500, 500], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[500, 500, 525, 525], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[525, 525, 550, 550], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[550, 550, 575, 575], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[575, 575, 600, 600], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[600, 600, 625, 625], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[625, 625, 650, 650], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[650, 650, 675, 675], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[675, 675, 700, 700], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[700, 700, 725, 725], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[725, 725, 750, 750], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[750, 750, 775, 775], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
       " [[775, 775, 800, 800], [1, 1, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0]],\n",
       " [[800, 800, 825, 825], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[825, 825, 850, 850], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[850, 850, 875, 875], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[875, 875, 900, 900], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[900, 900, 925, 925], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[925, 925, 950, 950], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[950, 950, 975, 975], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[975, 975, 1000, 1000], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1000, 1000, 1025, 1025], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1025, 1025, 1050, 1050], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1050, 1050, 1075, 1075], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1075, 1075, 1100, 1100], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1100, 1100, 1125, 1125], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1125, 1125, 1150, 1150], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1150, 1150, 1175, 1175], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1175, 1175, 1200, 1200], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1200, 1200, 1225, 1225], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1225, 1225, 1250, 1250], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1250, 1250, 1275, 1275], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1275, 1275, 1300, 1300], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1300, 1300, 1325, 1325], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1325, 1325, 1350, 1350], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1350, 1350, 1375, 1375], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1375, 1375, 1400, 1400], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1400, 1400, 1425, 1425], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1425, 1425, 1450, 1450], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
       " [[1450, 1450, 1475, 1475], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 1, 1]],\n",
       " [[1475, 1475, 1475, 1475], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_progression\n",
    "#[[gdu], [p], [d], [h]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a Gantt chart is produced based on the optimal $\\pi$, i.e., ```policy```. <font color='green'>Green</font> denotes the planting schedule, <font color='orange'>Orange</font> shows the detasseling schedule, and <font color='blue'>Blue</font> shows the harvesting schdule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gantt_chart(policy):\n",
    "    #figure \"gnt\" \n",
    "    fig, gnt = plt.subplots()\n",
    "\n",
    "    #setting axis limits \n",
    "    gnt.set_ylim(0, len(fields)) \n",
    "    gnt.set_xlim(0, T+1+1)\n",
    "\n",
    "    #setting labels for x-axis and y-axis \n",
    "    gnt.set_xlabel('Days') \n",
    "    gnt.set_ylabel('Fields')\n",
    "\n",
    "    #ticks and labels on y-axis \n",
    "    gnt.set_yticks(fields)\n",
    "    gnt.set_yticklabels(['1', '2', '3', '4'])\n",
    "\n",
    "    #setting graph attribute \n",
    "    gnt.grid(True)\n",
    "\n",
    "    for i in range(0, len(policy)):\n",
    "        u_p=policy[i][0]\n",
    "        u_d=policy[i][1]\n",
    "        u_h=policy[i][2]\n",
    "        if (np.any(np.array(u_p)!=0)):\n",
    "            for j in range(0, len(fields)):\n",
    "                if (u_p[j]==1):\n",
    "                    gnt.broken_barh([(i, 1)], (j, 1), facecolors=('tab:green'))\n",
    "        if (np.any(np.array(u_d)!=0)):\n",
    "            for j in range(0, len(fields)):\n",
    "                if (u_d[j]==1):\n",
    "                    gnt.broken_barh([(i, 1)], (j, 1), facecolors=('tab:orange'))\n",
    "        if (np.any(np.array(u_h)!=0)):\n",
    "            for j in range(0, len(fields)):\n",
    "                if (u_h[j]==1):\n",
    "                    gnt.broken_barh([(i, 1)], (j, 1), facecolors=('tab:blue'))\n",
    "    return fig, gnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 432x288 with 1 Axes>,\n",
       " <AxesSubplot:xlabel='Days', ylabel='Fields'>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gantt_chart(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
